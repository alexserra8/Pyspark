{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5490fb1",
   "metadata": {},
   "source": [
    "# 0. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b9aa088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pyspark\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c88b24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4da96670",
   "metadata": {},
   "source": [
    "# 1. Find Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94841531",
   "metadata": {},
   "source": [
    "Adding pyspark to sys.path at runtime using the library findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5afbe168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark-3.4.1-bin-hadoop3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad9a832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d5a7e20",
   "metadata": {},
   "source": [
    "# 2. Creating SparkContest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2e15c5",
   "metadata": {},
   "source": [
    "We use SparkContext to connect to the cluster manager to submit Spark jobs and know what resource manager (YARN, Mesos or Standalone) to communicate to. It is the heart of the Spark application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "755aabec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define some configuration and then we create the SparkContest with this configuration.\n",
    "conf= pyspark.SparkConf().setAppName(\"SparkApp\").setMaster(\"local\")\n",
    "sc=pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef488ced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "137f2e98",
   "metadata": {},
   "source": [
    "# 3. Example of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66ada8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example Test code\n",
    "numeric_val=sc.parallelize([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efed9c33",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 8, 27, 64]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_val.map(lambda x: x*x*x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09609582",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14196d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
