{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark as sp\n",
    "import findspark\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Find Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding pyspark to sys.path at runtime using the library findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\alexs\\\\Downloads\\\\spark-3.4.0-bin-hadoop3\\\\spark-3.4.0-bin-hadoop3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One aspect of the explanation why SparkSession is preferable over SparkContext in SparkSession Vs SparkContext battle is that SparkSession unifies all of Spark’s numerous contexts, removing the developer’s need to worry about generating separate contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] El sistema no puede encontrar el archivo especificado",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-336d961b5e6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Create the SparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmy_spark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#print the session\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_spark\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    475\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m                     \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m                     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m                     \u001b[1;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m                     \u001b[1;31m# by all sessions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    510\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 512\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    513\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    196\u001b[0m             )\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m             self._do_init(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    430\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                 \u001b[1;31m# preexec_fn not supported on Windows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m                 \u001b[0mproc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpopen_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m             \u001b[1;31m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    852\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[0;32m    853\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 854\u001b[1;33m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[0;32m    855\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    856\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1305\u001b[0m             \u001b[1;31m# Start the process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1307\u001b[1;33m                 hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n\u001b[0m\u001b[0;32m   1308\u001b[0m                                          \u001b[1;31m# no special security\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m                                          \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] El sistema no puede encontrar el archivo especificado"
     ]
    }
   ],
   "source": [
    "#Create the SparkSession\n",
    "my_spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "#print the session\n",
    "print(my_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dataset overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = my_spark.read.csv('2017_StPaul_MN_Real_Estate.csv', header=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select our dependent variable\n",
    "Y_df = df.select(['SalesClosePrice'])\n",
    "\n",
    "# Display summary statistics\n",
    "Y_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preprocessing data I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check data of the feature to predict \n",
    "df.select([\"SalesClosePrice\"]).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data type of SalesClosePrice to integer\n",
    "df = df.withColumn(\"SalesClosePrice\", df.SalesClosePrice.cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check again the type after converting it.\n",
    "df.select([\"SalesClosePrice\"]).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('SalesClosePrice').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modifying some types of key variables\n",
    "df = df.withColumn(\"AssessedValuation\", df.AssessedValuation.cast(\"double\"))\n",
    "df = df.withColumn(\"AssociationFee\", df.AssociationFee.cast(\"bigint\"))\n",
    "df = df.withColumn(\"SQFTBELOWGROUND\", df.SQFTBELOWGROUND.cast(\"bigint\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modifying name of column to capital letters\n",
    "required_dtypes = [('NO', 'bigint'),\n",
    " ('MLSID', 'string'),\n",
    " ('STREETNUMBERNUMERIC', 'bigint'),\n",
    " ('STREETADDRESS', 'string'),\n",
    " ('STREETNAME', 'string'),\n",
    " ('POSTALCODE', 'bigint'),\n",
    " ('STATEORPROVINCE', 'string'),\n",
    " ('CITY', 'string'),\n",
    " ('SALESCLOSEPRICE', 'bigint'),\n",
    " ('LISTDATE', 'string'),\n",
    " ('LISTPRICE', 'bigint'),\n",
    " ('LISTTYPE', 'string'),\n",
    " ('ORIGINALLISTPRICE', 'bigint'),\n",
    " ('PRICEPERTSFT', 'double'),\n",
    " ('FOUNDATIONSIZE', 'bigint'),\n",
    " ('FENCE', 'string'),\n",
    " ('MAPLETTER', 'string'),\n",
    " ('LOTSIZEDIMENSIONS', 'string'),\n",
    " ('SCHOOLDISTRICTNUMBER', 'string'),\n",
    " ('DAYSONMARKET', 'bigint'),\n",
    " ('OFFMARKETDATE', 'string'),\n",
    " ('FIREPLACES', 'bigint'),\n",
    " ('ROOMAREA4', 'string'),\n",
    " ('ROOMTYPE', 'string'),\n",
    " ('ROOF', 'string'),\n",
    " ('ROOMFLOOR4', 'string'),\n",
    " ('POTENTIALSHORTSALE', 'string'),\n",
    " ('POOLDESCRIPTION', 'string'),\n",
    " ('PDOM', 'bigint'),\n",
    " ('GARAGEDESCRIPTION', 'string'),\n",
    " ('SQFTABOVEGROUND', 'bigint'),\n",
    " ('TAXES', 'bigint'),\n",
    " ('ROOMFLOOR1', 'string'),\n",
    " ('ROOMAREA1', 'string'),\n",
    " ('TAXWITHASSESSMENTS', 'double'),\n",
    " ('TAXYEAR', 'bigint'),\n",
    " ('LIVINGAREA', 'bigint'),\n",
    " ('UNITNUMBER', 'string'),\n",
    " ('YEARBUILT', 'bigint'),\n",
    " ('ZONING', 'string'),\n",
    " ('STYLE', 'string'),\n",
    " ('ACRES', 'double'),\n",
    " ('COOLINGDESCRIPTION', 'string'),\n",
    " ('APPLIANCES', 'string'),\n",
    " ('BACKONMARKETDATE', 'double'),\n",
    " ('ROOMFAMILYCHAR', 'string'),\n",
    " ('ROOMAREA3', 'string'),\n",
    " ('EXTERIOR', 'string'),\n",
    " ('ROOMFLOOR3', 'string'),\n",
    " ('ROOMFLOOR2', 'string'),\n",
    " ('ROOMAREA2', 'string'),\n",
    " ('DININGROOMDESCRIPTION', 'string'),\n",
    " ('BASEMENT', 'string'),\n",
    " ('BATHSFULL', 'bigint'),\n",
    " ('BATHSHALF', 'bigint'),\n",
    " ('BATHQUARTER', 'bigint'),\n",
    " ('BATHSTHREEQUARTER', 'double'),\n",
    " ('CLASS', 'string'),\n",
    " ('BATHSTOTAL', 'bigint'),\n",
    " ('BATHDESC', 'string'),\n",
    " ('ROOMAREA5', 'string'),\n",
    " ('ROOMFLOOR5', 'string'),\n",
    " ('ROOMAREA6', 'string'),\n",
    " ('ROOMFLOOR6', 'string'),\n",
    " ('ROOMAREA7', 'string'),\n",
    " ('ROOMFLOOR7', 'string'),\n",
    " ('ROOMAREA8', 'string'),\n",
    " ('ROOMFLOOR8', 'string'),\n",
    " ('BEDROOMS', 'bigint'),\n",
    " ('SQFTBELOWGROUND', 'bigint'),\n",
    " ('ASSUMABLEMORTGAGE', 'string'),\n",
    " ('ASSOCIATIONFEE', 'bigint'),\n",
    " ('ASSESSMENTPENDING', 'string'),\n",
    " ('ASSESSEDVALUATION', 'double')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_columns = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = [c for c, d in required_dtypes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, o in zip(new_columns, old_columns): \n",
    "    df = df.withColumnRenamed(o, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the transformation of column names to capital letters\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for required_type, current_column in zip(required_dtypes, df.columns):\n",
    "    # since the required and current column names are the exact order we can do:\n",
    "    if required_type[1] != 'string':\n",
    "        df = df.withColumn(current_column, df[\"{:}\".format(current_column)].cast(required_type[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_columns = ['FOUNDATIONSIZE',\n",
    " 'DAYSONMARKET',\n",
    " 'FIREPLACES',\n",
    " 'PDOM',\n",
    " 'SQFTABOVEGROUND',\n",
    " 'TAXES',\n",
    " 'TAXWITHASSESSMENTS',\n",
    " 'TAXYEAR',\n",
    " 'LIVINGAREA',\n",
    " 'YEARBUILT',\n",
    " 'ACRES',\n",
    " 'BACKONMARKETDATE',\n",
    " 'BATHSFULL',\n",
    " 'BATHSHALF',\n",
    " 'BATHQUARTER',\n",
    " 'BATHSTHREEQUARTER',\n",
    " 'BATHSTOTAL',\n",
    " 'BEDROOMS',\n",
    " 'SQFTBELOWGROUND',\n",
    " 'ASSOCIATIONFEE',\n",
    " 'ASSESSEDVALUATION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name and value of col with max corr\n",
    "corr_max = 0\n",
    "corr_max_col = check_columns[0]\n",
    "\n",
    "# Loop to check all columns contained in list\n",
    "for col in check_columns:\n",
    "    # Check the correlation of a pair of columns\n",
    "    corr_val = df.corr(col, 'SALESCLOSEPRICE')\n",
    "    # Logic to compare corr_max with current corr_val\n",
    "    if corr_val > corr_max:\n",
    "        # Update the column name and corr value\n",
    "        corr_max = corr_val\n",
    "        corr_max_col = col\n",
    "\n",
    "print(corr_max_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a single column and sample and convert to pandas\n",
    "# sample 50% and not use replacement and setting the random seed to 42.\n",
    "sample_df = df.select(['LISTPRICE']).sample(False, .5, 42)\n",
    "pandas_df = sample_df.toPandas()\n",
    "\n",
    "# Plot distribution of pandas_df and display plot\n",
    "sns.distplot(pandas_df)\n",
    "plt.show()\n",
    "\n",
    "# Import skewness function\n",
    "from pyspark.sql.functions import skewness\n",
    "\n",
    "# Compute and print skewness of LISTPRICE\n",
    "print(df.agg({'LISTPRICE': 'skewness'}).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a the relevant columns and sample\n",
    "sample_df = df.select(['SALESCLOSEPRICE','LIVINGAREA']).sample(False, .5, 42)\n",
    "\n",
    "# Convert to pandas dataframe\n",
    "pandas_df = sample_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear model plot of pandas_df\n",
    "sns.lmplot(x='LIVINGAREA', y='SALESCLOSEPRICE', data=pandas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a relation. If Livingarea increase, the salescloseprice also increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Preprocessing data II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Dropping a list of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to remove from dataset\n",
    "cols_to_drop = ['STREETNUMBERNUMERIC', 'LOTSIZEDIMENSIONS']\n",
    "\n",
    "# Drop columns in list\n",
    "df = df.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Using text filters to remove records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect unique values in the column 'ASSUMABLEMORTGAGE'\n",
    "df.select(['ASSUMABLEMORTGAGE']).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('ASSUMABLEMORTGAGE').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of possible values containing 'yes'\n",
    "yes_values = ['Yes w/ Qualifying', 'Yes w/No Qualifying']\n",
    "\n",
    "# Filter the text values out of df but keep null values\n",
    "text_filter = ~df['ASSUMABLEMORTGAGE'].isin(yes_values) | df['ASSUMABLEMORTGAGE'].isNull()\n",
    "df = df.where(text_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We kept values that were null and values that where not in the list provided.\n",
    "df.groupBy('ASSUMABLEMORTGAGE').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Filtering numeric fields conditionally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing column to his log value.\n",
    "from pyspark.sql.functions import log\n",
    "df = df.withColumn('log_SalesClosePrice', log('SalesClosePrice'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a the relevant columns and sample\n",
    "sample_df = df.select(['log_SalesClosePrice'])\n",
    "\n",
    "# Convert to pandas dataframe\n",
    "pandas_df = sample_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=pandas_df[\"log_SalesClosePrice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, stddev\n",
    "\n",
    "# Calculate values used for outlier filtering\n",
    "mean_val = df.agg({'log_SalesClosePrice': 'mean'}).collect()[0][0]\n",
    "stddev_val = df.agg({'log_SalesClosePrice': 'stddev'}).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create three standard deviation (μ ± 3σ) lower and upper bounds for data\n",
    "low_bound = mean_val - (3 * stddev_val)\n",
    "hi_bound = mean_val + (3 * stddev_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data to fit between the lower and upper bounds\n",
    "df = df.where((df['log_SalesClosePrice'] < hi_bound) & (df['log_SalesClosePrice'] > low_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we have deleted outliers\n",
    "sample_df = df.select(['log_SalesClosePrice'])\n",
    "pandas_df = sample_df.toPandas()\n",
    "sns.boxplot(x=pandas_df[\"log_SalesClosePrice\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Custom Percentage Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a manual scaling of the Daysonmarket column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define max and min values and collect them\n",
    "max_days = df.agg({'DAYSONMARKET': 'max'}).collect()[0][0]\n",
    "min_days = df.agg({'DAYSONMARKET': 'min'}).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column based off the scaled data using the formula manually\n",
    "df = df.withColumn('percentage_scaled_days', \n",
    "                  round((df['DAYSONMARKET'] - min_days) / (max_days - min_days)) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc max and min for new column\n",
    "print(df.agg({'percentage_scaled_days': 'max'}).collect())\n",
    "print(df.agg({'percentage_scaled_days': 'min'}).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = df.select(['DAYSONMARKET'])\n",
    "pandas_df = sample_df.toPandas()\n",
    "sns.boxplot(x=pandas_df[\"DAYSONMARKET\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = df.select(['percentage_scaled_days'])\n",
    "pandas_df = sample_df.toPandas()\n",
    "sns.boxplot(x=pandas_df[\"percentage_scaled_days\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Scaling your scalers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a function that will scale automatically the desired features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaler(df, cols_to_scale):\n",
    "    # Takes a dataframe and list of columns to minmax scale. Returns a dataframe.\n",
    "    for col in cols_to_scale:\n",
    "        # Define min and max values and collect them\n",
    "        max_days = df.agg({col: 'max'}).collect()[0][0]\n",
    "        min_days = df.agg({col: 'min'}).collect()[0][0]\n",
    "        new_column_name = 'scaled_' + col\n",
    "        # Create a new column based off the scaled data\n",
    "        df = df.withColumn(new_column_name, \n",
    "                          (df[col] - min_days) / (max_days - min_days))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = min_max_scaler(df, ['FOUNDATIONSIZE', 'DAYSONMARKET', 'FIREPLACES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show that our data is now between 0 and 1\n",
    "df[['DAYSONMARKET', 'scaled_DAYSONMARKET']].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Correcting Right Skew Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the skewness\n",
    "print(df.agg({'YEARBUILT': 'skewness'}).collect())\n",
    "\n",
    "# Calculate the max year\n",
    "max_year = df.agg({'YEARBUILT': 'max'}).collect()[0][0]\n",
    "\n",
    "# Create a new column of reflected data\n",
    "df = df.withColumn('Reflect_YearBuilt', (max_year + 1) - df['YEARBUILT'])\n",
    "\n",
    "# Create a new column based reflected data\n",
    "df = df.withColumn('adj_yearbuilt', 1 / log(df['Reflect_YearBuilt']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7 Visualizing Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['APPLIANCES',\n",
    " 'BACKONMARKETDATE',\n",
    " 'ROOMFAMILYCHAR',\n",
    " 'BASEMENT',\n",
    " 'DININGROOMDESCRIPTION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(columns).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the dataframe and convert to Pandas\n",
    "sample_df = df.select(columns).sample(False, 0.5, 42)\n",
    "pandas_df = sample_df.toPandas()\n",
    "\n",
    "# Convert all values to T/F\n",
    "tf_df = pandas_df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot it\n",
    "sns.heatmap(data=tf_df)\n",
    "plt.xticks(rotation=30, fontsize=10)\n",
    "plt.yticks(rotation=0, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.8 Imputing Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing rows\n",
    "# In this case there aren't nulls. But the way to input would be the same.\n",
    "df.where(df['PDOM'].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean value\n",
    "col_mean = df.agg({'PDOM': 'mean'}).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing with the mean value for that column\n",
    "df.fillna(col_mean, subset=['PDOM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing rows after imputing \n",
    "df.where(df['PDOM'].isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.9 Calculate Missing Percents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create a function that will drop columns that have X percentage os missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_dropper(df, threshold):\n",
    "    # Takes a dataframe and threshold for missing values. Returns a dataframe.\n",
    "    total_records = df.count()\n",
    "    for col in df.columns:\n",
    "    # Calculate the percentage of missing values\n",
    "        missing = df.where(df[col].isNull()).count()\n",
    "        missing_percent = missing / total_records\n",
    "        # Drop column if percent of missing is more than threshold\n",
    "        if missing_percent > threshold:\n",
    "            df = df.drop(col)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of columns before\n",
    "print(len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are more than 60% missing\n",
    "df = column_dropper(df, .6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of columns after\n",
    "print(len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lot size in square feet\n",
    "acres_to_sqfeet = 43560\n",
    "df = df.withColumn('LOT_SIZE_SQFT', df['ACRES'] * acres_to_sqfeet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column YARD_SIZE\n",
    "df = df.withColumn('YARD_SIZE', df['LOT_SIZE_SQFT'] - df['FOUNDATIONSIZE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corr of ACRES vs SALESCLOSEPRICE\n",
    "print(\"Corr of ACRES vs SALESCLOSEPRICE: \" + str(df.corr('ACRES', 'SALESCLOSEPRICE')))\n",
    "# Corr of FOUNDATIONSIZE vs SALESCLOSEPRICE\n",
    "print(\"Corr of FOUNDATIONSIZE vs SALESCLOSEPRICE: \" + str(df.corr('FOUNDATIONSIZE', 'SALESCLOSEPRICE')))\n",
    "# Corr of YARD_SIZE vs SALESCLOSEPRICE\n",
    "print(\"Corr of YARD_SIZE vs SALESCLOSEPRICE: \" + str(df.corr('YARD_SIZE', 'SALESCLOSEPRICE')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSESSED_TO_LIST\n",
    "df = df.withColumn('ASSESSED_TO_LIST', df['ASSESSEDVALUATION'] / df['LISTPRICE'])\n",
    "df[['ASSESSEDVALUATION', 'LISTPRICE', 'ASSESSED_TO_LIST']].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAX_TO_LIST\n",
    "df = df.withColumn('TAX_TO_LIST', df['TAXES'] / df['LISTPRICE'])\n",
    "df[['TAX_TO_LIST', 'TAXES', 'LISTPRICE']].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BED_TO_BATHS\n",
    "df = df.withColumn('BED_TO_BATHS', df['BEDROOMS'] / df['BATHSTOTAL'])\n",
    "df[['BED_TO_BATHS', 'BEDROOMS', 'BATHSTOTAL']].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Deeper Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2(x, y):\n",
    "    return stats.pearsonr(x, y)[0] ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new feature by adding two features together\n",
    "df = df.withColumn('Total_SQFT', df['SQFTBELOWGROUND'] + df['SQFTABOVEGROUND'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional new feature using previously created feature\n",
    "df = df.withColumn('BATHS_PER_1000SQFT', df['BATHSTOTAL'] / (df['Total_SQFT'] / 1000))\n",
    "df[['BATHS_PER_1000SQFT']].describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample and create pandas dataframe\n",
    "pandas_df = df.sample(False, 0.5, 0).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df=pandas_df[pandas_df[\"BATHS_PER_1000SQFT\"]<1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear model plots\n",
    "sns.jointplot(x='Total_SQFT', y='SALESCLOSEPRICE', data=pandas_df)\n",
    "sns.jointplot(x='BATHS_PER_1000SQFT', y='SALESCLOSEPRICE', data=pandas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Time Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import needed functions\n",
    "from pyspark.sql.functions import to_date, dayofweek\n",
    "\n",
    "#Important to set this. If not, issues with datetime appear\n",
    "my_spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to date type\n",
    "df = df.withColumn('LISTDATE', to_date(df['LISTDATE'], format='MM/dd/yyyy HH:mm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the day of the week\n",
    "df = df.withColumn('List_Day_of_Week', dayofweek(df['LISTDATE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample and convert to pandas dataframe\n",
    "sample_df = df.sample(False, .5, 42).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot count plot of of day of week\n",
    "sns.countplot(x=\"List_Day_of_Week\", data=sample_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Joining On Time Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = dict(City=['LELM - Lake Elmo', 'MAPW - Maplewood','STP - Saint Paul','WB - Woodbury', \\\n",
    "                  'OAKD - Oakdale', 'LELM - Lake Elmo', 'MAPW - Maplewood', \\\n",
    "                  'STP - Saint Paul', 'WB - Woodbury', 'OAKD - Oakdale'],\n",
    "     MedianHomeValue=[401000, 193000, 172000, 291000, 210000, 385000, 187000, 162000, 277000, 192000],\n",
    "     Year= [2016,2016,2016,2016,2016,2015,2015,2015,2015, 2015])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df = my_spark.createDataFrame(df_price)\n",
    "price_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year\n",
    "\n",
    "# Create year column\n",
    "df = df.withColumn('list_year', year(df['LISTDATE']))\n",
    "\n",
    "# Adjust year to match\n",
    "df = df.withColumn('report_year', (df['list_year'] - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create join condition\n",
    "condition = [df['CITY'] == price_df['City'], df['report_year'] == price_df['year']]\n",
    "\n",
    "# Join the dataframes together\n",
    "df = df.join(price_df, on=condition, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect that new columns are available\n",
    "df[['MedianHomeValue']].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['MedianHomeValue']].describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 Date Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lag, datediff, to_date\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast data type\n",
    "mort_df = df.withColumn('DATE', to_date(df['LISTDATE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create window\n",
    "w = Window().orderBy(mort_df['DATE'])\n",
    "# Create lag column\n",
    "mort_df = mort_df.withColumn('DATE-1', lag(mort_df['DATE'], offset=1).over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mort_df.select(['DATE','DATE-1']).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate difference between date columns\n",
    "mort_df = mort_df.withColumn('Days_Between_Report', datediff(mort_df['DATE'], mort_df['DATE-1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mort_df.select(['DATE','DATE-1',\"Days_Between_Report\"]).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "mort_df.select('Days_Between_Report').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7 Extracting Text to New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed functions\n",
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boolean conditions for string matches\n",
    "has_attached_garage = df['GARAGEDESCRIPTION'].like('%Attached%')\n",
    "has_detached_garage = df['GARAGEDESCRIPTION'].like('%Detached%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional value assignment \n",
    "df = df.withColumn('has_attached_garage', (when(has_attached_garage, 1)\n",
    "                                          .when(has_detached_garage, 0)\n",
    "                                          .otherwise(None)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect results\n",
    "df[['GARAGEDESCRIPTION', 'has_attached_garage']].show(truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "https://github.com/ozlerhakan/datacamp/blob/master/Feature%20Engineering%20with%20PySpark/Feature%20Engineering%20with%20PySpark.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
