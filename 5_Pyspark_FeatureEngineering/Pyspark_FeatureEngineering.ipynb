{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2477772a",
   "metadata": {},
   "source": [
    "# 0. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "844cd465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark as sp\n",
    "import findspark\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95850c26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4da96670",
   "metadata": {},
   "source": [
    "# 1. Find Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94841531",
   "metadata": {},
   "source": [
    "Adding pyspark to sys.path at runtime using the library findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5afbe168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark-3.4.1-bin-hadoop3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baea63b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c1244b0",
   "metadata": {},
   "source": [
    "# 2. Creating SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e0579c",
   "metadata": {},
   "source": [
    "One aspect of the explanation why SparkSession is preferable over SparkContext in SparkSession Vs SparkContext battle is that SparkSession unifies all of Spark’s numerous contexts, removing the developer’s need to worry about generating separate contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a69faf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the SparkSession\n",
    "my_spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "#print the session\n",
    "print(my_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32719bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e549142b",
   "metadata": {},
   "source": [
    "# 3. Dataset overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4018e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = my_spark.read.csv('2017_StPaul_MN_Real_Estate.csv', header=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1525f080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select our dependent variable\n",
    "Y_df = df.select(['SalesClosePrice'])\n",
    "\n",
    "# Display summary statistics\n",
    "Y_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faecd03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0e81ef8",
   "metadata": {},
   "source": [
    "# 4. Preprocessing data I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94665501",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check data of the feature to predict \n",
    "df.select([\"SalesClosePrice\"]).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bcb2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data type of SalesClosePrice to integer\n",
    "df = df.withColumn(\"SalesClosePrice\", df.SalesClosePrice.cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e859bd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check again the type after converting it.\n",
    "df.select([\"SalesClosePrice\"]).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7d82c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('SalesClosePrice').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbfa459",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modifying some types of key variables\n",
    "df = df.withColumn(\"AssessedValuation\", df.AssessedValuation.cast(\"double\"))\n",
    "df = df.withColumn(\"AssociationFee\", df.AssociationFee.cast(\"bigint\"))\n",
    "df = df.withColumn(\"SQFTBELOWGROUND\", df.SQFTBELOWGROUND.cast(\"bigint\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ec7ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modifying name of column to capital letters\n",
    "required_dtypes = [('NO', 'bigint'),\n",
    " ('MLSID', 'string'),\n",
    " ('STREETNUMBERNUMERIC', 'bigint'),\n",
    " ('STREETADDRESS', 'string'),\n",
    " ('STREETNAME', 'string'),\n",
    " ('POSTALCODE', 'bigint'),\n",
    " ('STATEORPROVINCE', 'string'),\n",
    " ('CITY', 'string'),\n",
    " ('SALESCLOSEPRICE', 'bigint'),\n",
    " ('LISTDATE', 'string'),\n",
    " ('LISTPRICE', 'bigint'),\n",
    " ('LISTTYPE', 'string'),\n",
    " ('ORIGINALLISTPRICE', 'bigint'),\n",
    " ('PRICEPERTSFT', 'double'),\n",
    " ('FOUNDATIONSIZE', 'bigint'),\n",
    " ('FENCE', 'string'),\n",
    " ('MAPLETTER', 'string'),\n",
    " ('LOTSIZEDIMENSIONS', 'string'),\n",
    " ('SCHOOLDISTRICTNUMBER', 'string'),\n",
    " ('DAYSONMARKET', 'bigint'),\n",
    " ('OFFMARKETDATE', 'string'),\n",
    " ('FIREPLACES', 'bigint'),\n",
    " ('ROOMAREA4', 'string'),\n",
    " ('ROOMTYPE', 'string'),\n",
    " ('ROOF', 'string'),\n",
    " ('ROOMFLOOR4', 'string'),\n",
    " ('POTENTIALSHORTSALE', 'string'),\n",
    " ('POOLDESCRIPTION', 'string'),\n",
    " ('PDOM', 'bigint'),\n",
    " ('GARAGEDESCRIPTION', 'string'),\n",
    " ('SQFTABOVEGROUND', 'bigint'),\n",
    " ('TAXES', 'bigint'),\n",
    " ('ROOMFLOOR1', 'string'),\n",
    " ('ROOMAREA1', 'string'),\n",
    " ('TAXWITHASSESSMENTS', 'double'),\n",
    " ('TAXYEAR', 'bigint'),\n",
    " ('LIVINGAREA', 'bigint'),\n",
    " ('UNITNUMBER', 'string'),\n",
    " ('YEARBUILT', 'bigint'),\n",
    " ('ZONING', 'string'),\n",
    " ('STYLE', 'string'),\n",
    " ('ACRES', 'double'),\n",
    " ('COOLINGDESCRIPTION', 'string'),\n",
    " ('APPLIANCES', 'string'),\n",
    " ('BACKONMARKETDATE', 'double'),\n",
    " ('ROOMFAMILYCHAR', 'string'),\n",
    " ('ROOMAREA3', 'string'),\n",
    " ('EXTERIOR', 'string'),\n",
    " ('ROOMFLOOR3', 'string'),\n",
    " ('ROOMFLOOR2', 'string'),\n",
    " ('ROOMAREA2', 'string'),\n",
    " ('DININGROOMDESCRIPTION', 'string'),\n",
    " ('BASEMENT', 'string'),\n",
    " ('BATHSFULL', 'bigint'),\n",
    " ('BATHSHALF', 'bigint'),\n",
    " ('BATHQUARTER', 'bigint'),\n",
    " ('BATHSTHREEQUARTER', 'double'),\n",
    " ('CLASS', 'string'),\n",
    " ('BATHSTOTAL', 'bigint'),\n",
    " ('BATHDESC', 'string'),\n",
    " ('ROOMAREA5', 'string'),\n",
    " ('ROOMFLOOR5', 'string'),\n",
    " ('ROOMAREA6', 'string'),\n",
    " ('ROOMFLOOR6', 'string'),\n",
    " ('ROOMAREA7', 'string'),\n",
    " ('ROOMFLOOR7', 'string'),\n",
    " ('ROOMAREA8', 'string'),\n",
    " ('ROOMFLOOR8', 'string'),\n",
    " ('BEDROOMS', 'bigint'),\n",
    " ('SQFTBELOWGROUND', 'bigint'),\n",
    " ('ASSUMABLEMORTGAGE', 'string'),\n",
    " ('ASSOCIATIONFEE', 'bigint'),\n",
    " ('ASSESSMENTPENDING', 'string'),\n",
    " ('ASSESSEDVALUATION', 'double')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177fe297",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_columns = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be6fbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = [c for c, d in required_dtypes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a1aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, o in zip(new_columns, old_columns): \n",
    "    df = df.withColumnRenamed(o, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cfa2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the transformation of column names to capital letters\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d90f812",
   "metadata": {},
   "outputs": [],
   "source": [
    "for required_type, current_column in zip(required_dtypes, df.columns):\n",
    "    # since the required and current column names are the exact order we can do:\n",
    "    if required_type[1] != 'string':\n",
    "        df = df.withColumn(current_column, df[\"{:}\".format(current_column)].cast(required_type[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a437cc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6658e1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_columns = ['FOUNDATIONSIZE',\n",
    " 'DAYSONMARKET',\n",
    " 'FIREPLACES',\n",
    " 'PDOM',\n",
    " 'SQFTABOVEGROUND',\n",
    " 'TAXES',\n",
    " 'TAXWITHASSESSMENTS',\n",
    " 'TAXYEAR',\n",
    " 'LIVINGAREA',\n",
    " 'YEARBUILT',\n",
    " 'ACRES',\n",
    " 'BACKONMARKETDATE',\n",
    " 'BATHSFULL',\n",
    " 'BATHSHALF',\n",
    " 'BATHQUARTER',\n",
    " 'BATHSTHREEQUARTER',\n",
    " 'BATHSTOTAL',\n",
    " 'BEDROOMS',\n",
    " 'SQFTBELOWGROUND',\n",
    " 'ASSOCIATIONFEE',\n",
    " 'ASSESSEDVALUATION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c6d065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name and value of col with max corr\n",
    "corr_max = 0\n",
    "corr_max_col = check_columns[0]\n",
    "\n",
    "# Loop to check all columns contained in list\n",
    "for col in check_columns:\n",
    "    # Check the correlation of a pair of columns\n",
    "    corr_val = df.corr(col, 'SALESCLOSEPRICE')\n",
    "    # Logic to compare corr_max with current corr_val\n",
    "    if corr_val > corr_max:\n",
    "        # Update the column name and corr value\n",
    "        corr_max = corr_val\n",
    "        corr_max_col = col\n",
    "\n",
    "print(corr_max_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddf513a",
   "metadata": {},
   "source": [
    "# 5. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0bf562",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93e8017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a single column and sample and convert to pandas\n",
    "# sample 50% and not use replacement and setting the random seed to 42.\n",
    "sample_df = df.select(['LISTPRICE']).sample(False, .5, 42)\n",
    "pandas_df = sample_df.toPandas()\n",
    "\n",
    "# Plot distribution of pandas_df and display plot\n",
    "sns.distplot(pandas_df)\n",
    "plt.show()\n",
    "\n",
    "# Import skewness function\n",
    "from pyspark.sql.functions import skewness\n",
    "\n",
    "# Compute and print skewness of LISTPRICE\n",
    "print(df.agg({'LISTPRICE': 'skewness'}).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f510e0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a the relevant columns and sample\n",
    "sample_df = df.select(['SALESCLOSEPRICE','LIVINGAREA']).sample(False, .5, 42)\n",
    "\n",
    "# Convert to pandas dataframe\n",
    "pandas_df = sample_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943cc961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear model plot of pandas_df\n",
    "sns.lmplot(x='LIVINGAREA', y='SALESCLOSEPRICE', data=pandas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c33ca8",
   "metadata": {},
   "source": [
    "We can see a relation. If Livingarea increase, the salescloseprice also increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7973b53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0a8ca35",
   "metadata": {},
   "source": [
    "# 6. Preprocessing data II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e483a12",
   "metadata": {},
   "source": [
    "### 6.1 Dropping a list of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687a7f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to remove from dataset\n",
    "cols_to_drop = ['STREETNUMBERNUMERIC', 'LOTSIZEDIMENSIONS']\n",
    "\n",
    "# Drop columns in list\n",
    "df = df.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c3fc33",
   "metadata": {},
   "source": [
    "### 6.2 Using text filters to remove records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf00a688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect unique values in the column 'ASSUMABLEMORTGAGE'\n",
    "df.select(['ASSUMABLEMORTGAGE']).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6c26c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('ASSUMABLEMORTGAGE').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d875ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cc9a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of possible values containing 'yes'\n",
    "yes_values = ['Yes w/ Qualifying', 'Yes w/No Qualifying']\n",
    "\n",
    "# Filter the text values out of df but keep null values\n",
    "text_filter = ~df['ASSUMABLEMORTGAGE'].isin(yes_values) | df['ASSUMABLEMORTGAGE'].isNull()\n",
    "df = df.where(text_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f222e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bdbef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We kept values that were null and values that where not in the list provided.\n",
    "df.groupBy('ASSUMABLEMORTGAGE').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09627ec8",
   "metadata": {},
   "source": [
    "### 6.3 Filtering numeric fields conditionally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be9aa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing column to his log value.\n",
    "from pyspark.sql.functions import log\n",
    "df = df.withColumn('log_SalesClosePrice', log('SalesClosePrice'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66e34cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a the relevant columns and sample\n",
    "sample_df = df.select(['log_SalesClosePrice'])\n",
    "\n",
    "# Convert to pandas dataframe\n",
    "pandas_df = sample_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b896c890",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a024d522",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=pandas_df[\"log_SalesClosePrice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03a6412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, stddev\n",
    "\n",
    "# Calculate values used for outlier filtering\n",
    "mean_val = df.agg({'log_SalesClosePrice': 'mean'}).collect()[0][0]\n",
    "stddev_val = df.agg({'log_SalesClosePrice': 'stddev'}).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4bc8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create three standard deviation (μ ± 3σ) lower and upper bounds for data\n",
    "low_bound = mean_val - (3 * stddev_val)\n",
    "hi_bound = mean_val + (3 * stddev_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15167591",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d90befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data to fit between the lower and upper bounds\n",
    "df = df.where((df['log_SalesClosePrice'] < hi_bound) & (df['log_SalesClosePrice'] > low_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad709242",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f3a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we have deleted outliers\n",
    "sample_df = df.select(['log_SalesClosePrice'])\n",
    "pandas_df = sample_df.toPandas()\n",
    "sns.boxplot(x=pandas_df[\"log_SalesClosePrice\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5e5ca7",
   "metadata": {},
   "source": [
    "### 6.4 Custom Percentage Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6585211c",
   "metadata": {},
   "source": [
    "Creating a manual scaling of the Daysonmarket column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2c3f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c4772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define max and min values and collect them\n",
    "max_days = df.agg({'DAYSONMARKET': 'max'}).collect()[0][0]\n",
    "min_days = df.agg({'DAYSONMARKET': 'min'}).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e892f897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column based off the scaled data using the formula manually\n",
    "df = df.withColumn('percentage_scaled_days', \n",
    "                  round((df['DAYSONMARKET'] - min_days) / (max_days - min_days)) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ea90ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calc max and min for new column\n",
    "print(df.agg({'percentage_scaled_days': 'max'}).collect())\n",
    "print(df.agg({'percentage_scaled_days': 'min'}).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9e73a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = df.select(['DAYSONMARKET'])\n",
    "pandas_df = sample_df.toPandas()\n",
    "sns.boxplot(x=pandas_df[\"DAYSONMARKET\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f44465",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = df.select(['percentage_scaled_days'])\n",
    "pandas_df = sample_df.toPandas()\n",
    "sns.boxplot(x=pandas_df[\"percentage_scaled_days\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6225d2",
   "metadata": {},
   "source": [
    "### 6.5 Scaling your scalers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe13c56",
   "metadata": {},
   "source": [
    "Creating a function that will scale automatically the desired features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b11eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaler(df, cols_to_scale):\n",
    "    # Takes a dataframe and list of columns to minmax scale. Returns a dataframe.\n",
    "    for col in cols_to_scale:\n",
    "        # Define min and max values and collect them\n",
    "        max_days = df.agg({col: 'max'}).collect()[0][0]\n",
    "        min_days = df.agg({col: 'min'}).collect()[0][0]\n",
    "        new_column_name = 'scaled_' + col\n",
    "        # Create a new column based off the scaled data\n",
    "        df = df.withColumn(new_column_name, \n",
    "                          (df[col] - min_days) / (max_days - min_days))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ce2dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = min_max_scaler(df, ['FOUNDATIONSIZE', 'DAYSONMARKET', 'FIREPLACES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4423f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show that our data is now between 0 and 1\n",
    "df[['DAYSONMARKET', 'scaled_DAYSONMARKET']].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9812abd8",
   "metadata": {},
   "source": [
    "### 6.6 Correcting Right Skew Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955800dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the skewness\n",
    "print(df.agg({'YEARBUILT': 'skewness'}).collect())\n",
    "\n",
    "# Calculate the max year\n",
    "max_year = df.agg({'YEARBUILT': 'max'}).collect()[0][0]\n",
    "\n",
    "# Create a new column of reflected data\n",
    "df = df.withColumn('Reflect_YearBuilt', (max_year + 1) - df['YEARBUILT'])\n",
    "\n",
    "# Create a new column based reflected data\n",
    "df = df.withColumn('adj_yearbuilt', 1 / log(df['Reflect_YearBuilt']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7e3829",
   "metadata": {},
   "source": [
    "### 6.7 Visualizing Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c4da37",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['APPLIANCES',\n",
    " 'BACKONMARKETDATE',\n",
    " 'ROOMFAMILYCHAR',\n",
    " 'BASEMENT',\n",
    " 'DININGROOMDESCRIPTION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39be15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(columns).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797949fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the dataframe and convert to Pandas\n",
    "sample_df = df.select(columns).sample(False, 0.5, 42)\n",
    "pandas_df = sample_df.toPandas()\n",
    "\n",
    "# Convert all values to T/F\n",
    "tf_df = pandas_df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1066b4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot it\n",
    "sns.heatmap(data=tf_df)\n",
    "plt.xticks(rotation=30, fontsize=10)\n",
    "plt.yticks(rotation=0, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85721843",
   "metadata": {},
   "source": [
    "### 6.8 Imputing Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3440690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing rows\n",
    "df.where(df['PDOM'].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542fada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean value\n",
    "col_mean = df.agg({'PDOM': 'mean'}).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af811b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing with the mean value for that column\n",
    "df.fillna(col_mean, subset=['PDOM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54822939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing rows after imputing \n",
    "df.where(df['PDOM'].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041d44bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f2f22a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb2a310",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1789c487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fae474a",
   "metadata": {},
   "source": [
    "Sources:\n",
    "https://github.com/ozlerhakan/datacamp/blob/master/Feature%20Engineering%20with%20PySpark/Feature%20Engineering%20with%20PySpark.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed927729",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
